{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2058653/3782452170.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  expert_traj = np.array([np.array(i) for i in expert_traj])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import benchmark\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from itertools import combinations\n",
    "# import gym\n",
    "from random import randrange\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.distributions.categorical import Categorical\n",
    "from env_32_none import abs_dis, ele_list_gen, cor_func_all, swap_step, ideal_cor_func\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "path = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/SQSdrl/'\n",
    "expert_traj = np.load(path+'ExpertTraj/tar8_step2_108act_32atom_none.npy', allow_pickle=True)\n",
    "expert_traj = np.array([np.array(i) for i in expert_traj])\n",
    "ind_1nn = np.load(path+'struc_info/ind_1nn.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def draw_3d(ind_raw):\n",
    "    ind_raw = np.array(ind_raw)\n",
    "    plt.rcParams[\"figure.figsize\"] = [5, 5]\n",
    "    # plt.rcParams[\"figure.autolayout\"] = True\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(ind_raw[:,0], ind_raw[:,1], ind_raw[:,2], alpha = 0.5, c = 'r')\n",
    "    plt.show()\n",
    "\n",
    "def embed_ele(ele_list):\n",
    "    len_ele = len(ele_list[0])\n",
    "    ele_emb = torch.zeros(len_ele, 3)\n",
    "    for i in range(len_ele):\n",
    "        ele_emb[i][int(ele_list[i])+1] = 1\n",
    "    \n",
    "    return ele_emb\n",
    "\n",
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(\"Created Directory : \", directory)\n",
    "    else:\n",
    "        print(\"Directory already existed : \", directory)\n",
    "    return directory\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Reward(nn.Module):\n",
    "    def __init__(self, a_tilt):\n",
    "        super(Reward, self).__init__()\n",
    "\n",
    "        self.a_tilt = a_tilt\n",
    "        self.output = nn.Sequential(\n",
    "            layer_init(nn.Linear(34, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 1), std=0.1),\n",
    "        )\n",
    "        self.gcn1 = nn.Sequential(\n",
    "            layer_init(nn.Linear(34, 34)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.gcn2 = nn.Sequential(\n",
    "            layer_init(nn.Linear(34, 34)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.gcn3 = nn.Sequential(\n",
    "            layer_init(nn.Linear(34, 34)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, s_a):\n",
    "        \n",
    "        input_state = torch.mm(self.a_tilt, s_a.T).T\n",
    "        s = self.gcn1(input_state)\n",
    "        s = torch.mm(self.a_tilt, s.T).T\n",
    "        s = self.gcn2(s)\n",
    "        s = torch.mm(self.a_tilt, s.T).T\n",
    "        s = self.gcn3(s)\n",
    "        s = torch.mm(self.a_tilt, s.T).T\n",
    "        r = self.output(s)\n",
    "\n",
    "        return r\n",
    "\n",
    "class Inv_r():\n",
    "    def __init__(self):\n",
    "        self.lr = 1e-5\n",
    "        self.mini_batchsize = 64\n",
    "        self.mini_batchsize_e = 1024\n",
    "        self.epoch = 8\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        a_tilt_a = np.load('/media/wz/7AD631A4D6316195/Projects/SQS_drl/graph/fcc_32/a_tilt_a34.npy')\n",
    "        a_tilt_a = torch.tensor(a_tilt_a, dtype=torch.float).to(self.device)\n",
    "        self.reward = Reward(a_tilt_a).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.reward.parameters(), lr=self.lr)\n",
    "    \n",
    "    def reward_c(self, s_a):\n",
    "        reward = self.reward(s_a).detach().cpu().numpy().flatten()\n",
    "        return reward\n",
    "\n",
    "    def update(self, p_traj, p_traj_p, e_traj):\n",
    "        for _ in range(self.epoch):\n",
    "            p_ind = np.random.choice(len(p_traj), self.mini_batchsize, replace=True)\n",
    "            s_a = np.array([p_traj[i] for i in p_ind])\n",
    "            a_prob = np.array([p_traj_p[i] for i in p_ind])\n",
    "            s_a_e = np.random.choice(e_traj, self.mini_batchsize_e, replace=True) # every traj can be taken as a batch\n",
    "\n",
    "            r_e, r_p, w_j = 0, 0, []\n",
    "\n",
    "            for i in range(self.mini_batchsize):\n",
    "                s_a_ = torch.FloatTensor(s_a[i]).to(self.device)\n",
    "                a_prob_ = torch.FloatTensor(a_prob[i]).to(self.device)\n",
    "                # print(torch.prod(a_prob_))\n",
    "                #*Attention that p_term may exhibit NAN without normalization\n",
    "                w_ = torch.exp(torch.sum(self.reward(s_a_)))/torch.prod(a_prob_*108)\n",
    "                # print(self.reward(s_a_))\n",
    "                w_ = w_.detach().cpu()\n",
    "                w_j.append(w_)\n",
    "            \n",
    "            w_j = torch.FloatTensor(w_j).to(self.device)\n",
    "            sum_wj = torch.sum(w_j)\n",
    "            # *Expert reward\n",
    "            for i in range(self.mini_batchsize_e):\n",
    "                s_a_e_ = torch.FloatTensor(s_a_e[i]).to(self.device)\n",
    "                r_e += torch.sum(self.reward(s_a_e_))\n",
    "            \n",
    "            for i in range(self.mini_batchsize):\n",
    "                s_a_i = torch.FloatTensor(s_a[i]).to(self.device)\n",
    "                r_p += torch.sum(self.reward(s_a_i))*(w_j[i]/sum_wj)\n",
    "            # print(s_a_e_, s_a_i)\n",
    "            p_term, e_term = r_p, r_e/self.mini_batchsize_e\n",
    "            reward_loss = p_term - e_term\n",
    "            # print([p_term, e_term])\n",
    "            # Update actor\n",
    "            self.optimizer.zero_grad()\n",
    "            reward_loss.backward()\n",
    "            clip_grad_norm_(self.reward.parameters(), 1)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, a_tilt):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.action_s = 108\n",
    "        self.a_tilt = a_tilt\n",
    "        self.output = nn.Sequential(\n",
    "            layer_init(nn.Linear(32, 128)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, self.action_s), std=0.01)\n",
    "        )\n",
    "        self.gcn1 = nn.Sequential(\n",
    "            layer_init(nn.Linear(32, 32)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.gcn2 = nn.Sequential(\n",
    "            layer_init(nn.Linear(32, 32)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.gcn3 = nn.Sequential(\n",
    "            layer_init(nn.Linear(32, 32)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, atom_list, mask, replace_tensor):\n",
    "        \n",
    "        mask = mask.type(torch.BoolTensor).to(device)\n",
    "        input_state = torch.mm(self.a_tilt, atom_list.T).T\n",
    "        s = self.gcn1(input_state)\n",
    "        s = torch.mm(self.a_tilt, s.T).T\n",
    "        s = self.gcn2(s)\n",
    "        s = torch.mm(self.a_tilt, s.T).T\n",
    "        s = self.gcn3(s)\n",
    "        s = torch.mm(self.a_tilt, s.T).T\n",
    "        s = self.output(s)\n",
    "        # print(mask, s, replace_tensor)\n",
    "        s = torch.where(mask, s, replace_tensor)\n",
    "        #*Using softmax\n",
    "        s = F.softmax(s, dim=1)\n",
    "        # print(a_prob)\n",
    "        return s\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    # *value function \n",
    "    def __init__(self, a_tilt):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.a_tilt = a_tilt\n",
    "        self.output = nn.Sequential(\n",
    "            layer_init(nn.Linear(32, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1)\n",
    "        )\n",
    "        self.gcn1 = nn.Sequential(\n",
    "            layer_init(nn.Linear(32, 32)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.gcn2 = nn.Sequential(\n",
    "            layer_init(nn.Linear(32, 32)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.gcn3 = nn.Sequential(\n",
    "            layer_init(nn.Linear(32, 32)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, atom_list):\n",
    "\n",
    "        input_state = torch.mm(self.a_tilt, atom_list.T).T\n",
    "        s = self.gcn1(input_state)\n",
    "        s = torch.mm(self.a_tilt, s.T).T\n",
    "        s = self.gcn2(s)\n",
    "        s = torch.mm(self.a_tilt, s.T).T\n",
    "        s = self.gcn3(s)\n",
    "        s = torch.mm(self.a_tilt, s.T).T\n",
    "        v_s = self.output(s)\n",
    "    \n",
    "        return v_s\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, batch_size, state_dim, device):\n",
    "        self.s = np.zeros((batch_size, state_dim))\n",
    "        self.mask = np.zeros((batch_size, 108))\n",
    "        self.a = np.zeros((batch_size, 1))\n",
    "        self.a_prob = np.zeros((batch_size, 1))\n",
    "        self.r = np.zeros((batch_size, 1))\n",
    "        self.s_ = np.zeros((batch_size, state_dim))\n",
    "        self.dw = np.zeros((batch_size, 1))\n",
    "        self.done = np.zeros((batch_size, 1))\n",
    "        self.count = 0\n",
    "        self.device = device\n",
    "\n",
    "    def store(self, s, mask, a, a_prob, r, s_, dw, done):\n",
    "        self.s[self.count] = s\n",
    "        self.mask[self.count] = mask\n",
    "        self.a[self.count] = a\n",
    "        self.a_prob[self.count] = a_prob\n",
    "        self.r[self.count] = r\n",
    "        self.s_[self.count] = s_\n",
    "        self.dw[self.count] = dw\n",
    "        self.done[self.count] = done\n",
    "        self.count += 1\n",
    "\n",
    "    def numpy_to_tensor(self):\n",
    "        s = torch.tensor(self.s, dtype=torch.float).to(self.device)\n",
    "        mask = torch.tensor(self.mask, dtype=torch.float).to(self.device)\n",
    "        a = torch.tensor(self.a, dtype=torch.long).to(self.device)\n",
    "        a_prob = torch.tensor(self.a_prob, dtype=torch.float).to(self.device)\n",
    "        r = torch.tensor(self.r, dtype=torch.float).to(self.device)\n",
    "        s_ = torch.tensor(self.s_, dtype=torch.float).to(self.device)\n",
    "        dw = torch.tensor(self.dw, dtype=torch.float).to(self.device)\n",
    "        done = torch.tensor(self.done, dtype=torch.float).to(self.device)\n",
    "\n",
    "        return s, mask, a, a_prob, r, s_, dw, done\n",
    "\n",
    "class PPO():\n",
    "    def __init__(self, state_dim, action_dim, batch_size):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.mini_batch_size = 512\n",
    "        self.eps_clip = 0.1\n",
    "        self.K_epochs = 16\n",
    "        self.GAMMA = 0.93  # discount factor\n",
    "        self.LAMDA = 0.95  # GAE parameter\n",
    "        self.lr = 1e-5\n",
    "        self.alpha = 0.02\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #*\\widetilde{L} from GCN paper arXiv:1609.02907\n",
    "        self.a_tilt = np.load('/media/wz/7AD631A4D6316195/Projects/SQS_drl/graph/fcc_32/a_tilt.npy')\n",
    "        self.a_tilt = torch.tensor(self.a_tilt, dtype=torch.float).to(self.device)\n",
    "        # self.action_list = np.array([[i,j] for i, j in combinations(np.arange(32), 2)])\n",
    "        self.action_list = ind_1nn\n",
    "        self.replacetensor = torch.tensor(-1e+8, device=self.device)\n",
    "        self.actor = Actor(self.a_tilt).to(self.device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.lr)\n",
    "\n",
    "        self.critic = Critic(self.a_tilt).to(self.device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.lr)\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def swap(self, state, a_ind, mode='unsqueeze'):\n",
    "        a_chosen = self.action_list[a_ind]\n",
    "        a1, a2 = a_chosen\n",
    "        if mode == 'unsqueeze':\n",
    "            state[0][a1], state[0][a2] = state[0][a2], state[0][a1]\n",
    "        elif mode == 'squeeze':\n",
    "            state[a1], state[a2] = state[a2], state[a1]\n",
    "        return state\n",
    "\n",
    "    def choose_action(self, s, mask, deterministic):\n",
    "        s = torch.unsqueeze(torch.tensor(s, dtype=torch.float), 0).to(self.device)\n",
    "        mask = torch.unsqueeze(torch.tensor(mask, dtype=torch.float), 0)\n",
    "        mask = mask.type(torch.BoolTensor).to(self.device)\n",
    "        prob_weights = self.actor(s, mask, self.replacetensor).detach().cpu().numpy().flatten()  # probability distribution(numpy)\n",
    "        if deterministic:\n",
    "            a = np.argmax(prob_weights)\n",
    "            return a\n",
    "        else: \n",
    "            #*pi(s'|s)*alpha(s'|s)\n",
    "            #*Better not to consider the acceptance ratio.\n",
    "            # while True:\n",
    "                # alpha = np.random.random()\n",
    "            a = np.random.choice(range(self.action_dim), p=prob_weights)  # Sample the action according to the probability distribution;\n",
    "            a_prob = prob_weights[a]  # The corresponding probability\n",
    "                # acceptance = a_prob/prob_inv\n",
    "                # if alpha <= np.min([1, acceptance]):\n",
    "                #     break\n",
    "        return a, a_prob\n",
    "\n",
    "    def update(self, memory, memory_):\n",
    "        s, mask, a, a_prob, r, s_, dw, done = memory.numpy_to_tensor()\n",
    "\n",
    "        adv = []\n",
    "        gae = 0\n",
    "        s = s\n",
    "        s_ = s_\n",
    "\n",
    "        with torch.no_grad():  #* Adv and td error\n",
    "            vs = self.critic(s)\n",
    "            vs_ = self.critic(s_)\n",
    "            deltas = r + self.GAMMA * (1.0 - dw) * vs_ - vs\n",
    "            for delta, d in zip(reversed(deltas.flatten().cpu().numpy()), reversed(done.flatten().cpu().numpy())):\n",
    "                gae = delta + self.GAMMA * self.LAMDA * gae * (1.0 - d)\n",
    "                adv.insert(0, gae)\n",
    "            adv = torch.tensor(adv, dtype=torch.float).view(-1, 1).to(self.device)\n",
    "            td_target = adv + vs\n",
    "            adv = ((adv - adv.mean()) / (adv.std() + 1e-5))\n",
    "\n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Random sampling and no repetition. 'False' indicates that training will continue even if the number of samples in the last time is less than mini_batch_size\n",
    "            for index in BatchSampler(SubsetRandomSampler(range(self.batch_size)), self.mini_batch_size, False):\n",
    "                # dist_now = Categorical(probs=self.actor(s[index], mask[index], self.replacetensor))\n",
    "                # dist_entropy = dist_now.entropy().view(-1, 1)\n",
    "                # a_logprob_now = dist_now.log_prob(a[index].squeeze()).view(-1, 1)\n",
    "                # ratios = torch.exp(a_logprob_now - a_prob[index])\n",
    "                a_prob_now = self.actor(s[index], mask[index], self.replacetensor).gather(1, a[index])\n",
    "                # a_prob_inv = self.actor(s_[index]).gather(1, a[index])\n",
    "\n",
    "                # accept_for = torch.clamp(a_prob_now/a_prob_inv, max=1.0)\n",
    "                # accept_inv = torch.clamp(a_prob[index]/prob_inv[index], max=1.0)\n",
    "\n",
    "                ratios = a_prob_now / a_prob[index]\n",
    "\n",
    "                surr1 = ratios * adv[index]  # Gradient of a_prob_now\n",
    "                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * adv[index]\n",
    "                # actor_loss = -torch.min(surr1, surr2) - self.alpha * dist_entropy\n",
    "                actor_loss = -torch.min(surr1, surr2)\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.mean().backward()\n",
    "                clip_grad_norm_(self.actor.parameters(), 1)\n",
    "                self.actor_optimizer.step()\n",
    "                # loss between value network and td errors (advantage)\n",
    "                v_s = self.critic(s[index])\n",
    "                critic_loss = self.MseLoss(td_target[index], v_s)\n",
    "\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                clip_grad_norm_(self.critic.parameters(), 1)\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set random seed\n",
    "    seed = 886\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    state_dim = 32\n",
    "    action_dim = 108\n",
    "    target_val = 8\n",
    "    max_episode_steps = 50  # Maximum number of steps per episode\n",
    "    SQS_name = 'SQS_prime'\n",
    "    print(f\"state_dim={state_dim}\")\n",
    "    print(f\"action_dim={action_dim}\")\n",
    "    print(f\"max_episode_steps={max_episode_steps}\")\n",
    "\n",
    "    batch_size = 2048\n",
    "    max_train_steps = 5e6  # Maximum number of training steps.\n",
    "    total_steps = 0  # Record the total steps during the training.\n",
    "    r_list = [] # Record the reward\n",
    "    done_num, done_ratio, step_per_episode = 0, [], [] # Time for done.\n",
    "    mean_spe = [] # Record the average of steps till done per episode\n",
    "    traj, traj_p = [], [] # Trajectory & the corresponding probability generated by policy\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # action_list = np.array([[i,j] for i, j in combinations(np.arange(32), 2)])\n",
    "    action_list = ind_1nn\n",
    "\n",
    "    memory = Memory(batch_size, state_dim, device)\n",
    "    # memory_ = Memory(batch_size, state_dim, device)\n",
    "    #*Inv reward\n",
    "    reward = Inv_r()\n",
    "    \n",
    "    agent = PPO(state_dim, action_dim, batch_size)\n",
    "    \n",
    "    date = '220926_none_1'\n",
    "    path_save = path+f'runs/{date}'\n",
    "    writer = SummaryWriter(log_dir=path_save)\n",
    "    while total_steps < max_train_steps:\n",
    "        while True:\n",
    "            ac_1 = 1/3*np.random.rand() + 1/6\n",
    "            ac_2 = 1/3*np.random.rand() + 1/6\n",
    "            # ac_1, ac_2 = 1/3, 1/3\n",
    "            ac_3 = 1 - ac_1 - ac_2\n",
    "            ideal_cor = ideal_cor_func(ac_1, ac_2, ind_1nn)\n",
    "            if 1/6 <= ac_3 < 1/2:\n",
    "                #*Atomic contents\n",
    "                s = ele_list_gen(ac_1, ac_2, ac_3)\n",
    "                cor_func_raw = cor_func_all(s, ideal_cor)\n",
    "                if cor_func_raw >= 20:\n",
    "                    break\n",
    "\n",
    "        episode_steps = 0\n",
    "        r_total = 0\n",
    "        done = False\n",
    "        traj_, traj_p_ = [], []\n",
    "        action_mask = np.ones(action_dim)\n",
    "\n",
    "        while not done:\n",
    "            episode_steps += 1\n",
    "            a, a_prob = agent.choose_action(s, action_mask, deterministic=False)  # action and the corresponding probability\n",
    "            a_ = action_list[a]\n",
    "            s_, r, _, done = swap_step(a_, s, target_val=target_val, \n",
    "                                step=episode_steps, ideal=ideal_cor, reward_type='determine')\n",
    "\n",
    "            if done and episode_steps <= max_episode_steps:\n",
    "                dw = True\n",
    "            else:\n",
    "                dw = False\n",
    "            #*Inv reward\n",
    "            s_a = np.concatenate([s, a_/31], 0)\n",
    "            r = reward.reward_c(torch.unsqueeze(torch.FloatTensor(s_a).to(device), 0))\n",
    "            traj_.append(s_a)\n",
    "            traj_p_.append(a_prob)\n",
    "\n",
    "            # if memory_.count < batch_size:\n",
    "            #     memory_.store(s, a, a_prob, r, s_, dw, done)\n",
    "            # elif done_num % 20 == 0 and memory_.count == batch_size:\n",
    "            #     memory_.count = 0\n",
    "\n",
    "            memory.store(s, action_mask, a, a_prob, r, s_, dw, done)\n",
    "            action_mask = np.ones(action_dim)\n",
    "            action_mask[a] = 0\n",
    "            s = s_\n",
    "            r_total += r\n",
    "            # When the number of transitions in memory reaches batch_size,then update\n",
    "            if memory.count == batch_size:\n",
    "                # if memory_.count == batch_size:\n",
    "                #     agent.update(memory, memory_) \n",
    "                # elif memory_.count < batch_size:\n",
    "                agent.update(memory, memory) \n",
    "                memory.count = 0\n",
    "\n",
    "            r_list.append(r)\n",
    "            total_steps += 1\n",
    "        \n",
    "        done_num += 1\n",
    "        #*Inv reward, store the trans. prob. and traj.\n",
    "        traj.append(np.array(traj_))\n",
    "        traj_p.append(np.array(traj_p_))\n",
    "        # if len(traj) >= 64 and done_num % 500 == 0:\n",
    "        #     reward.update(np.array(traj), np.array(traj_p), expert_traj)\n",
    "\n",
    "        #*For matplotlib visualization\n",
    "        # r_list.append(r_total)\n",
    "        # done_ratio.append(done_num/total_steps)\n",
    "        step_per_episode.append(episode_steps)\n",
    "        if done_num > 1000:\n",
    "            mean_spe_latest = np.mean(step_per_episode[-1000:])\n",
    "            # mean_spe.append(mean_spe_latest)\n",
    "        elif done_num <= 1000:\n",
    "            mean_spe_latest = np.mean(step_per_episode)\n",
    "            # mean_spe.append(mean_spe_latest)\n",
    "        # mean_spe_extend = [mean_spe[-1]]*len(mean_spe)\n",
    "        # if done_num % 100 == 0:\n",
    "        clear_output(True)\n",
    "            # fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 10))\n",
    "            # ax1.plot(r_list)\n",
    "            # ax2.set_ylim([0.05, 0.15])\n",
    "            # ax2.plot(done_ratio)\n",
    "            \n",
    "            # ax3.plot(step_per_episode, zorder=0)\n",
    "            # ax3.plot(mean_spe_extend, zorder=5)\n",
    "            # ax3.plot(mean_spe, zorder=4)\n",
    "            # plt.suptitle(f'Num steps: {total_steps}')\n",
    "            # plt.show()\n",
    "        \n",
    "        writer.add_scalar(\"Done/Total step ratio\", done_num/total_steps, done_num)\n",
    "        writer.add_scalar(\"Reward\", r_total, done_num)\n",
    "        writer.add_scalar(\"step per episode\", episode_steps, done_num)\n",
    "        # writer.add_scalar(\"step per episode\", mean_spe_latest, done_num)\n",
    "\n",
    "    fc_layer = '32, 4'\n",
    "    Activation =  'Tanh'\n",
    "\n",
    "    k_epoch = agent.K_epochs\n",
    "    lr, mini_bs, gamma, clip, lamda = agent.lr, agent.mini_batch_size, agent.GAMMA, agent.eps_clip, agent.LAMDA\n",
    "\n",
    "    f= open(path_save+'/hyperparams.txt',\"w+\")\n",
    "    f.write('comment: Inv r\\n')\n",
    "    f.write(f'FC layers: {fc_layer}\\n')\n",
    "    f.write(f'Actication: {Activation}\\n')\n",
    "    f.write(f'lr: {lr}, mini_bs: {mini_bs}, bs: {batch_size}\\n')\n",
    "    f.write(f'gamma: {gamma}, clip: {clip}, lamda: {lamda}\\n')\n",
    "    f.write(f'k_epoch: {k_epoch}\\n')\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    np.save(path_save+'/done_ratio', done_ratio)\n",
    "    # plt.savefig(path_save+'/performance.png')\n",
    "    torch.save(agent.actor.state_dict(), path_save+'/actor_params.pth')\n",
    "    torch.save(agent.critic.state_dict(), path_save+'/critic_params.pth')\n",
    "    writer.close()\n",
    "    # shutil.copyfile(\"\", \n",
    "    #         path+'/config.ipynb')\n",
    "    # shutil.copyfile(\"\", \n",
    "    #         path+'/utils.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_mc = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/SQSdrl/runs/demo_mc_none_108'\n",
    "writer = SummaryWriter(log_dir=path_save_mc)\n",
    "\n",
    "mc_demo = np.load('/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/SQSdrl/struc_info/MCresults/puremc_108_none3_actmask.npy')\n",
    "for i in range(len(mc_demo)):\n",
    "    writer.add_scalar(\"Done/Total step ratio\", mc_demo[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_mc = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/SQSdrl/runs/demo_mc_e_108'\n",
    "writer = SummaryWriter(log_dir=path_save_mc)\n",
    "\n",
    "mc_demo = np.load('/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/SQSdrl/struc_info/MCresults/puremc_108.npy')\n",
    "for i in range(len(mc_demo)):\n",
    "    writer.add_scalar(\"Done/Total step ratio\", mc_demo[i], i)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68475d8e8ba7c27bff5b0c1dcce162ecdafd8f583568d2d03f898fe272d0ccc7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
